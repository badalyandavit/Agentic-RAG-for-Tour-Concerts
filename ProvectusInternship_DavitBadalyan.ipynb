{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "!pip -q install langchain faiss-cpu sentence-transformers transformers openai langchain_community google-search-results streamlit colabcode pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUYtlfJfxLKM",
        "outputId": "878a546e-1202-41b9-cb40-2bc3b3cc8567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m391.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring version 0.13.1 of uvicorn since it has invalid metadata:\n",
            "Requested uvicorn==0.13.1 from https://files.pythonhosted.org/packages/ef/67/546c35e9fffb585ea0608ba3bdcafe17ae402e304367203d0b08d6c23051/uvicorn-0.13.1-py3-none-any.whl (from colabcode) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    python-dotenv (>=0.13.*) ; extra == 'standard'\n",
            "                   ~~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring version 0.13.1 of uvicorn since it has invalid metadata:\n",
            "Requested uvicorn==0.13.1 from https://files.pythonhosted.org/packages/ef/67/546c35e9fffb585ea0608ba3bdcafe17ae402e304367203d0b08d6c23051/uvicorn-0.13.1-py3-none-any.whl (from colabcode) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    python-dotenv (>=0.13.*) ; extra == 'standard'\n",
            "                   ~~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring version 0.13.1 of uvicorn since it has invalid metadata:\n",
            "Requested uvicorn==0.13.1 from https://files.pythonhosted.org/packages/ef/67/546c35e9fffb585ea0608ba3bdcafe17ae402e304367203d0b08d6c23051/uvicorn-0.13.1-py3-none-any.whl (from colabcode) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    python-dotenv (>=0.13.*) ; extra == 'standard'\n",
            "                   ~~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
        "    level=logging.INFO\n",
        ")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "os.environ[\"SERPAPI_API_KEY\"] = getpass.getpass(\"Enter your SERP API key: \")\n",
        "os.environ[\"NGROK_API_KEY\"] = getpass.getpass(\"Enter your NGROK API key: \")\n",
        "\n",
        "logging.info(\"Successfully retrieved and set all API keys.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOQ0pK0CJ4vw",
        "outputId": "a81c7901-644e-4bf9-bce4-aa8080e8c2e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Enter your SERP API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Enter your NGROK API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tour_rag_service.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import re, os, json, logging, datetime\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s %(levelname)s  %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "log = logging.getLogger(\"TourRAG\")\n",
        "\n",
        "_DOMAIN_RE = re.compile(\n",
        "    r\"\\b(concerts?|tours?|gigs?|venues?|setlists?|support acts?|festivals?|arenas?|stadiums?)\\b\",\n",
        "    re.I,\n",
        ")\n",
        "_ARTIST_RE = re.compile(\n",
        "    r\"\\b(tours?|concerts?|bands?|artists?|singers?|gigs?|shows?)\\b\", re.I\n",
        ")\n",
        "\n",
        "def _is_tour_query(txt: str) -> bool:\n",
        "    return bool(_DOMAIN_RE.search(txt))\n",
        "\n",
        "def _looks_like_artist_query(txt: str) -> bool:\n",
        "    return bool(_ARTIST_RE.search(txt)) and len(txt.split()) <= 15\n",
        "\n",
        "def _extract_artist(q: str) -> str:\n",
        "    cleaned = re.sub(r\"[^\\w\\s]\", \" \", q)\n",
        "    cleaned = re.sub(\n",
        "        r\"\\b(upcoming|concerts?|tours?|shows?|dates?|gigs?)\\b\", \" \", cleaned, flags=re.I\n",
        "    )\n",
        "    words = [w for w in cleaned.split() if len(w) >= 2]\n",
        "    return \" \".join(words).strip()\n",
        "\n",
        "def _snip(acc: str, res) -> str:\n",
        "    if isinstance(res, list):\n",
        "        text = \"\\n\".join(d.get(\"snippet\", \"\") for d in res if isinstance(d, dict))\n",
        "    else:\n",
        "        text = str(res)\n",
        "    return (acc + \"\\n\" + text).strip()\n",
        "\n",
        "class _VecDB:\n",
        "    def __init__(self, dim: int):\n",
        "        self.idx = faiss.IndexFlatIP(dim)\n",
        "        self.txt: List[str]  = []\n",
        "        self.meta: List[Dict] = []\n",
        "\n",
        "    def add(self, emb: np.ndarray, text: str, meta: Dict):\n",
        "        self.idx.add(emb.astype(\"float32\")[None])\n",
        "        self.txt.append(text)\n",
        "        self.meta.append(meta)\n",
        "\n",
        "    def search_all(self, emb: np.ndarray, min_sim: float = 0.25):\n",
        "        if not self.idx.ntotal:\n",
        "            return []\n",
        "        D, I = self.idx.search(emb.astype(\"float32\")[None], self.idx.ntotal)\n",
        "        hits = [\n",
        "            (float(sim), self.txt[i], self.meta[i])\n",
        "            for sim, i in zip(D[0], I[0])\n",
        "            if i != -1 and sim >= min_sim\n",
        "        ]\n",
        "        hits.sort(key=lambda t: t[0], reverse=True)\n",
        "        return [(txt, meta) for _, txt, meta in hits]\n",
        "\n",
        "class TourRAG:\n",
        "    def __init__(self, save_data=True):\n",
        "        self.emb   = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
        "        self.summ  = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "        self.llm   = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "        self.vs    = _VecDB(self.emb.get_sentence_embedding_dimension())\n",
        "        self.save_data = save_data\n",
        "\n",
        "        try:\n",
        "            from langchain_community.utilities import SerpAPIWrapper\n",
        "            self.web = SerpAPIWrapper() if os.getenv(\"SERPAPI_API_KEY\") else None\n",
        "        except Exception:\n",
        "            self.web = None\n",
        "\n",
        "        self.train_root = Path(\"training_data\")\n",
        "        if self.save_data:\n",
        "            self.train_root.mkdir(exist_ok=True)\n",
        "\n",
        "    def _embed(self, txt: str) -> np.ndarray:\n",
        "        return self.emb.encode(txt, convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "    def _summarize(self, txt: str, max_tokens: int = 130) -> str:\n",
        "        words = txt.split()\n",
        "        if len(words) <= 950:\n",
        "            return self.summ(\n",
        "                txt, max_length=max_tokens, min_length=20, do_sample=False\n",
        "            )[0][\"summary_text\"]\n",
        "        out = []\n",
        "        for i in range(0, len(words), 950):\n",
        "            chunk = \" \".join(words[i : i + 950])\n",
        "            out.append(\n",
        "                self.summ(\n",
        "                    chunk, max_length=max_tokens, min_length=20, do_sample=False\n",
        "                )[0][\"summary_text\"]\n",
        "            )\n",
        "        return \" \".join(out)\n",
        "\n",
        "    def ingest(self, text: str) -> str:\n",
        "        if not _is_tour_query(text):\n",
        "            raise ValueError(\"Sorry, I cannot ingest documents with other themes.\")\n",
        "        summary = self._summarize(text)\n",
        "        self.vs.add(self._embed(summary), summary, {\"doc_id\": self.vs.idx.ntotal})\n",
        "        return summary\n",
        "\n",
        "    def answer(self, query: str) -> str:\n",
        "        # 1ï¸âƒ£  try RAG\n",
        "        rag_hits = self.vs.search_all(self._embed(query))\n",
        "        if rag_hits:\n",
        "            ctx = \"\\n\\n\".join(f\"- {txt}\" for txt, _ in rag_hits[:20])\n",
        "            guard = (\n",
        "                \"You are a QA assistant. Use ONLY the facts that appear in the context.\\n\"\n",
        "                \"If the context lacks the answer, respond with exactly:\\n\"\n",
        "                \"    I don't know.\\n\"\n",
        "                \"====  CONTEXT  ====\\n\"\n",
        "                f\"{ctx}\\n\"\n",
        "                \"==== END CONTEXT ====\"\n",
        "            )\n",
        "            ans = self.llm([SystemMessage(content=guard), HumanMessage(content=query)]).content.strip()\n",
        "            if ans.lower() != \"i don't know.\":\n",
        "                self._save_training_sample(\n",
        "                    query=query,\n",
        "                    answer=ans,\n",
        "                    meta={\"mode\": \"rag\", \"context\": ctx}\n",
        "                )\n",
        "                return ans\n",
        "\n",
        "        # 2ï¸âƒ£  not a tour query => bail early\n",
        "        if not _is_tour_query(query) and not _looks_like_artist_query(query):\n",
        "            return \"I don't know.\"\n",
        "\n",
        "        # 3ï¸âƒ£  artist lookup on the Web\n",
        "        if _looks_like_artist_query(query) and self.web is not None:\n",
        "            ans, snippets = self._answer_via_web(query)\n",
        "            if ans.lower() != \"i don't know.\":\n",
        "                self._save_training_sample(\n",
        "                    query=query,\n",
        "                    answer=ans,\n",
        "                    meta={\"mode\": \"web\", \"snippets\": snippets}\n",
        "                )\n",
        "            return ans\n",
        "\n",
        "        return \"I don't know.\"\n",
        "\n",
        "    def _answer_via_web(self, query: str) -> Tuple[str, str]:\n",
        "        artist  = _extract_artist(query)\n",
        "        variants = [\n",
        "            f\"{artist} tour dates 2025 2026\",\n",
        "            f\"{artist} upcoming concerts 2025\",\n",
        "            f\"{artist} world tour 2024 2025\",\n",
        "            query,\n",
        "        ]\n",
        "\n",
        "        snippets = \"\"\n",
        "        for q in variants:\n",
        "            try:\n",
        "                res = self.web.run(q)\n",
        "                snippets = _snip(snippets, res)\n",
        "                if len(snippets) >= 600:\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                log.error(f\"SerpAPI error on '{q}': {e}\")\n",
        "\n",
        "        if not snippets:\n",
        "            return \"I don't know.\", snippets\n",
        "\n",
        "        guard = (\n",
        "            \"Using ONLY the information inside WEBÂ SNIPPETS, answer the user.\\n\"\n",
        "            \"If the snippets do not contain the answer, respond with exactly:\\n\"\n",
        "            \"    I don't know.\\n\"\n",
        "            \"====  WEBÂ SNIPPETS  ====\\n\"\n",
        "            f\"{snippets}\\n\"\n",
        "            \"==== ENDÂ SNIPPETS ====\"\n",
        "        )\n",
        "        ans = self.llm([SystemMessage(content=guard), HumanMessage(content=query)]).content.strip()\n",
        "        return ans, snippets\n",
        "\n",
        "    def _save_training_sample(self, *, query: str, answer: str, meta: Dict):\n",
        "        \"\"\"Persist a single (<system>Â +Â <user>Â +Â assistant) triple.\n",
        "\n",
        "        Output is wrapped in <output> tags to ease postâ€‘processing.\n",
        "        \"\"\"\n",
        "        if not self.save_data:\n",
        "          return\n",
        "\n",
        "        iso_date = datetime.date.today().isoformat()\n",
        "        doc_dir  = self.train_root / iso_date\n",
        "        doc_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        idx = len(list(doc_dir.glob(\"sample_*.json\")))\n",
        "        fname = doc_dir / f\"sample_{idx:03d}.json\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": query}\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": f\"<output>{answer}</output>\"}\n",
        "                ]\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        record = {\n",
        "            \"messages\": messages,\n",
        "            \"meta\": meta,\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            with fname.open(\"w\", encoding=\"utf-8\") as fh:\n",
        "                json.dump(record, fh, indent=2, ensure_ascii=False)\n",
        "            log.info(f\"Training sample saved â†’ {fname}\")\n",
        "        except Exception as e:\n",
        "            log.error(f\"Could not write training sample: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O40owjvJRJSp",
        "outputId": "5aa249b6-4ca0-4694-f086-8a7d0d4fdb7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tour_rag_service.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from tour_rag_service import TourRAG\n",
        "\n",
        "st.set_page_config(page_title=\"ðŸŽ¤ Concertâ€‘Tour RAG\", page_icon=\"ðŸŽ¸\", layout=\"wide\")\n",
        "\n",
        "rag = st.session_state.get(\"rag\") or TourRAG()\n",
        "st.session_state[\"rag\"] = rag\n",
        "\n",
        "st.title(\"ðŸŽ¤ Concertâ€‘Tour RAG Assistant\")\n",
        "\n",
        "with st.form(key=\"io_form\"):\n",
        "    uploaded_file = st.file_uploader(\n",
        "        \"ðŸ“„ Upload document (optional)\",\n",
        "        type=[\"txt\"],\n",
        "        help=\"Select a .txt or .md file to ingest\"\n",
        "    )\n",
        "    doc_text = \"\"\n",
        "    if uploaded_file is not None:\n",
        "        raw = uploaded_file.read()\n",
        "        doc_text = raw.decode(\"utf-8\", errors=\"replace\")\n",
        "        st.text_area(\n",
        "            \"ðŸ“„ Document preview\",\n",
        "            value=doc_text,\n",
        "            height=220,\n",
        "            disabled=True\n",
        "        )\n",
        "\n",
        "    query_text = st.text_input(\n",
        "        \"ðŸ’¬ Question (optional)\",\n",
        "        placeholder=\"Ask about tour dates, venues, setâ€‘lists â€¦\"\n",
        "    )\n",
        "\n",
        "    submitted = st.form_submit_button(\"Submit\")\n",
        "\n",
        "if submitted:\n",
        "    if not doc_text.strip() and not query_text.strip():\n",
        "        st.warning(\"Please upload a document, enter a question, or both.\")\n",
        "        st.stop()\n",
        "\n",
        "    # ---------- 1) ingest (if a document was supplied) ----------\n",
        "    if doc_text.strip():\n",
        "        try:\n",
        "            summary = rag.ingest(doc_text)\n",
        "            st.success(\"âœ… Document ingested successfully!\")\n",
        "            with st.expander(\"ðŸ“‘ Generated summary\"):\n",
        "                st.write(summary)\n",
        "        except ValueError as e:\n",
        "            st.error(str(e))\n",
        "            st.stop()\n",
        "\n",
        "    # ---------- 2) questionâ€‘answering (if a question was supplied) ----------\n",
        "    if query_text.strip():\n",
        "        answer = rag.answer(query_text)\n",
        "        st.markdown(\"**ðŸŽ¯ Answer:**\")\n",
        "        st.write(answer)\n",
        "\n",
        "st.caption(\n",
        "    \"â„¹ï¸ Answers are sourced from your ingested documents; \"\n",
        "    \"if the information is absent *and* the prompt looks like an artist name, \"\n",
        "    \"the assistant does a live web lookup.\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdN9knQJICcb",
        "outputId": "b78056f5-dec4-41d7-c761-0ddc98f12186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import conf, ngrok\n",
        "import subprocess, shlex, time, atexit\n",
        "\n",
        "# authenticate\n",
        "conf.get_default().auth_token = os.getenv(\"NGROK_API_KEY\")\n",
        "\n",
        "public_url = ngrok.connect(8501, \"http\")\n",
        "print(f\"ðŸŒÂ Streamlit UI available at: {public_url}\")\n",
        "\n",
        "# start the Streamlit server\n",
        "proc = subprocess.Popen(\n",
        "    shlex.split(\"streamlit run app.py --server.port 8501 --server.headless true\")\n",
        ")\n",
        "\n",
        "# ensure clean shutdown\n",
        "def _cleanup():\n",
        "    proc.terminate()\n",
        "    ngrok.kill()\n",
        "\n",
        "atexit.register(_cleanup)\n",
        "\n",
        "# keep notebook alive\n",
        "while True:\n",
        "    time.sleep(60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "whYGdYZkGQ8c",
        "outputId": "b41b00d8-3e0e-44ec-9d51-37daf42f39fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŒÂ Streamlit UI available at: NgrokTunnel: \"https://e690-34-82-81-90.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-806cab46b949>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# keep notebook alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}